{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963c8478-f33a-4217-86ea-3c42911c0b1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-09-29T20:50:23.143242Z",
     "iopub.status.busy": "2022-09-29T20:50:23.143242Z",
     "iopub.status.idle": "2022-09-29T20:50:28.736300Z",
     "shell.execute_reply": "2022-09-29T20:50:28.735798Z",
     "shell.execute_reply.started": "2022-09-29T20:50:23.143242Z"
    },
    "id": "BrcXpPzTSszu",
    "outputId": "4af118e9-57bc-4313-f930-9c2348d2f219",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\GCM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
    "import spacy\n",
    "import crosslingual_coreference\n",
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from spacy import Language\n",
    "from typing import List\n",
    "from spacy.tokens import Doc, Span\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63af62f8-d9d2-43f8-8464-f73aaee85eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T20:50:28.739298Z",
     "iopub.status.busy": "2022-09-29T20:50:28.738797Z",
     "iopub.status.idle": "2022-09-29T20:50:28.767799Z",
     "shell.execute_reply": "2022-09-29T20:50:28.766800Z",
     "shell.execute_reply.started": "2022-09-29T20:50:28.739298Z"
    },
    "id": "cGsgtd8lxhVo"
   },
   "outputs": [],
   "source": [
    "def call_wiki_api(item):\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "        data = requests.get(url).json()\n",
    "        # Return the first id (Could upgrade this in the future)\n",
    "        return data['search'][0]['id']\n",
    "    except:\n",
    "        return 'id-less'\n",
    "\n",
    "def extract_triplets(text):\n",
    "    \"\"\"\n",
    "    Function to parse the generated text and extract the triplets\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": 0,\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "            Doc.set_extension(\"rel\", default={})\n",
    "\n",
    "    def get_wiki_id(self, item: str):\n",
    "        mapping = self.entity_mapping.get(item)\n",
    "        if mapping:\n",
    "            return mapping\n",
    "        else:\n",
    "            res = call_wiki_api(item)\n",
    "            self.entity_mapping[item] = res\n",
    "            return res\n",
    "\n",
    "    \n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "        output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "        extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "        extracted_triplets = extract_triplets(extracted_text[0])\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "                continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1241c1d-d664-4897-9af5-87facc47b718",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-09-29T20:50:28.769798Z",
     "iopub.status.busy": "2022-09-29T20:50:28.769298Z",
     "iopub.status.idle": "2022-09-29T20:50:58.983831Z",
     "shell.execute_reply": "2022-09-29T20:50:58.982831Z",
     "shell.execute_reply.started": "2022-09-29T20:50:28.769798Z"
    },
    "id": "SSTDfj3nSSca",
    "outputId": "0dbdbb3c-7a6c-46a3-8b9e-dc89f8af7486",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\GCM\\AppData\\Local\\Temp\\tmp6ba4c3j1\\config.json as plain json\n",
      "Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RebelComponent at 0x1b7b2ffa0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 0 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_trf', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\"xx_coref\", \n",
    "    config={\"chunk_size\": 2500, \n",
    "            \"chunk_overlap\": 2, \n",
    "            \"device\": DEVICE}\n",
    "              )\n",
    "\n",
    "# Define rel extraction model\n",
    "rel_ext = spacy.load('en_core_web_trf', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", \n",
    "                 config={\n",
    "                     'device':DEVICE,\n",
    "                     'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b0524a-a834-4e5f-995a-e20c9f5a2b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T20:50:58.985332Z",
     "iopub.status.busy": "2022-09-29T20:50:58.985332Z",
     "iopub.status.idle": "2022-09-29T20:50:58.999830Z",
     "shell.execute_reply": "2022-09-29T20:50:58.998331Z",
     "shell.execute_reply.started": "2022-09-29T20:50:58.985332Z"
    }
   },
   "outputs": [],
   "source": [
    "# crosslingual_coreference implementation\n",
    "def coref_res(text_series):\n",
    "    coref_text_series = text_series.apply(lambda x : coref(x)._.resolved_text)\n",
    "    return(coref_text_series)\n",
    "\n",
    "# # choose minilm for speed/memory and info_xlm for accuracy\n",
    "# predictor = Predictor(\n",
    "#     language=\"en_core_web_sm\", device=-1, model_name=\"minilm\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c9b2b0-fc17-4219-a3f2-7814f71e1c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T20:50:59.001831Z",
     "iopub.status.busy": "2022-09-29T20:50:59.001331Z",
     "iopub.status.idle": "2022-09-29T20:50:59.479492Z",
     "shell.execute_reply": "2022-09-29T20:50:59.478489Z",
     "shell.execute_reply.started": "2022-09-29T20:50:59.001831Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def link_entities_text(text):\n",
    "    ent_rel_lst = list(rel_ext(text)._.rel.values())\n",
    "    entity_df = pd.DataFrame()\n",
    "    rel_lst = []\n",
    "    head_text_lst = []\n",
    "    head_wiki_id_lst = []\n",
    "    tail_text_lst = []\n",
    "    tail_wiki_id_lst = []\n",
    "    for i in range(len(ent_rel_lst)):\n",
    "        rel_lst.append(ent_rel_lst[i]['relation'])\n",
    "        head_text_lst.append(ent_rel_lst[i]['head_span']['text'])\n",
    "        head_wiki_id_lst.append(ent_rel_lst[i]['head_span']['id'])\n",
    "        tail_text_lst.append(ent_rel_lst[i]['tail_span']['text'])\n",
    "        tail_wiki_id_lst.append(ent_rel_lst[i]['tail_span']['id'])\n",
    "    entity_df['head_text'] = head_text_lst\n",
    "    entity_df['head_wiki_id'] = head_wiki_id_lst\n",
    "    entity_df['relation'] = rel_lst\n",
    "    entity_df['tail_text'] = tail_text_lst\n",
    "    entity_df['tail_wiki_id'] = tail_wiki_id_lst\n",
    "    return(entity_df)\n",
    "\n",
    "def link_entities(text_series):\n",
    "    entity_df_series = text_series.apply(lambda x : link_entities_text(x))\n",
    "    return(entity_df_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b2f9c9-0580-4015-a0d5-f61738c54de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T20:50:59.480992Z",
     "iopub.status.busy": "2022-09-29T20:50:59.480491Z",
     "iopub.status.idle": "2022-09-29T20:51:01.057331Z",
     "shell.execute_reply": "2022-09-29T20:51:01.056331Z",
     "shell.execute_reply.started": "2022-09-29T20:50:59.480992Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data_df = pd.read_csv('E:\\GIT_REPOS\\LAB\\Literature_summary\\Papers\\\\scopus_bark_ambrosia_beetles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3babaf-6763-4262-9bab-116ad5a4419e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T21:57:24.985835Z",
     "iopub.status.busy": "2022-09-29T21:57:24.985335Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coref_series = coref_res(text_series=data_df[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e67d72-1de9-46da-ba7c-79829ee02a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T20:51:01.059839Z",
     "iopub.status.busy": "2022-09-29T20:51:01.059331Z",
     "iopub.status.idle": "2022-09-29T21:21:13.460472Z",
     "shell.execute_reply": "2022-09-29T21:21:13.458972Z",
     "shell.execute_reply.started": "2022-09-29T20:51:01.059839Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "[E040] Attempt to access token at 795, max length 791.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# create coref dataframe\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m coref_series \u001b[38;5;241m=\u001b[39m \u001b[43mcoref_res\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAbstract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished coref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m link_entities_series \u001b[38;5;241m=\u001b[39m link_entities(text_series\u001b[38;5;241m=\u001b[39mcoref_series)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mcoref_res\u001b[1;34m(text_series)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoref_res\u001b[39m(text_series):\n\u001b[1;32m----> 3\u001b[0m     coref_text_series \u001b[38;5;241m=\u001b[39m \u001b[43mtext_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolved_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(coref_text_series)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\pandas\\core\\series.py:4774\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4665\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4666\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4669\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4670\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4671\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4672\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4673\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4772\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4773\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\pandas\\core\\apply.py:1100\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\pandas\\core\\apply.py:1151\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1150\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1151\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2919\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mcoref_res.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoref_res\u001b[39m(text_series):\n\u001b[1;32m----> 3\u001b[0m     coref_text_series \u001b[38;5;241m=\u001b[39m text_series\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mcoref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mresolved_text)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(coref_text_series)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\spacy\\language.py:1025\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1025\u001b[0m     \u001b[43merror_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE005\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\spacy\\util.py:1658\u001b[0m, in \u001b[0;36mraise_error\u001b[1;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[1;32m-> 1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\spacy\\language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1020\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\crosslingual_coreference\\CrossLingualPredictorSpacy.py:33\u001b[0m, in \u001b[0;36mCrossLingualPredictorSpacy.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc: Doc) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    predict the class for a spacy Doc\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m        Doc: spacy doc with ._.cats key-class proba-value dict\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_prediction_to_doc(doc, prediction)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\crosslingual_coreference\\CrossLingualPredictor.py:128\u001b[0m, in \u001b[0;36mCrossLingualPredictor.predict\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    123\u001b[0m     corrected_clusters\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    124\u001b[0m         [[[num \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(doc_lengths[: idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m span] \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m clus] \u001b[38;5;28;01mfor\u001b[39;00m clus \u001b[38;5;129;01min\u001b[39;00m doc_clus]\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    127\u001b[0m merged_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_clusters(corrected_clusters)\n\u001b[1;32m--> 128\u001b[0m resolved_text, heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_corefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerged_clusters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m prediction \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclusters\u001b[39m\u001b[38;5;124m\"\u001b[39m: merged_clusters, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolved_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: resolved_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m: heads}\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\crosslingual_coreference\\CorefResolver.py:111\u001b[0m, in \u001b[0;36mCorefResolver.replace_corefs\u001b[1;34m(self, document, clusters)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m coref \u001b[38;5;129;01min\u001b[39;00m cluster:\n\u001b[0;32m    110\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m coref \u001b[38;5;241m!=\u001b[39m mention \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_containing_other_spans(coref, all_spans):\n\u001b[1;32m--> 111\u001b[0m                 \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_logic_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmention_span\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(resolved), cluster_heads\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\crosslingual_coreference\\CorefResolver.py:26\u001b[0m, in \u001b[0;36mCorefResolver.core_logic_part\u001b[1;34m(document, coref, resolved, mention_span)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcore_logic_part\u001b[39m(document: Doc, coref: List[\u001b[38;5;28mint\u001b[39m], resolved: List[\u001b[38;5;28mstr\u001b[39m], mention_span: Span):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    If the last token of the mention is a possessive pronoun, then add an apostrophe and an s to the mention.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Otherwise, just add the last token to the mention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    :return: The resolved list is being returned.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     final_token \u001b[38;5;241m=\u001b[39m \u001b[43mdocument\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcoref\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     27\u001b[0m     final_token_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(final_token\u001b[38;5;241m.\u001b[39mtag_)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     28\u001b[0m     test_token_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\spacy\\tokens\\doc.pyx:482\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\kg\\lib\\site-packages\\spacy\\tokens\\token.pxd:23\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.cinit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: [E040] Attempt to access token at 795, max length 791."
     ]
    }
   ],
   "source": [
    "link_entities_series = link_entities(text_series=coref_series)\n",
    "all_entities_df = pd.concat(link_entities_series.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a4770-4438-49fb-8936-e5b4c11aa017",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-29T21:21:13.460973Z",
     "iopub.status.idle": "2022-09-29T21:21:13.461974Z",
     "shell.execute_reply": "2022-09-29T21:21:13.461474Z",
     "shell.execute_reply.started": "2022-09-29T21:21:13.461474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_entities_df.to_csv('entity_edgelist.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
