{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec393a90-e8ce-42f0-ab43-dc3e0e9c5389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T17:12:36.613590Z",
     "iopub.status.busy": "2022-09-28T17:12:36.613090Z",
     "iopub.status.idle": "2022-09-28T17:12:36.624592Z",
     "shell.execute_reply": "2022-09-28T17:12:36.621591Z",
     "shell.execute_reply.started": "2022-09-28T17:12:36.613590Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install crosslingual-coreference==0.2.3 spacy-transformers==1.1.5 wikipedia neo4j transformers==4.18.0\n",
    "# !pip install --upgrade google-cloud-storage\n",
    "# !pip install \n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba11d43-3014-4762-b52a-24c948aaedf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T17:12:36.626592Z",
     "iopub.status.busy": "2022-09-28T17:12:36.626095Z",
     "iopub.status.idle": "2022-09-28T17:12:40.854582Z",
     "shell.execute_reply": "2022-09-28T17:12:40.852581Z",
     "shell.execute_reply.started": "2022-09-28T17:12:36.626592Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\GCM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\Users\\GCM\\anaconda3\\envs\\LIT\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Language' has no attribute 'factory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7380\\1189511377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcrosslingual_coreference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\LIT\\lib\\site-packages\\crosslingual_coreference\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m @Language.factory(\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"xx_coref\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     default_config={\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Language' has no attribute 'factory'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import crosslingual_coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cefb89-3767-4471-a63a-fb4d1e792551",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-28T17:12:40.855582Z",
     "iopub.status.idle": "2022-09-28T17:12:40.856080Z",
     "shell.execute_reply": "2022-09-28T17:12:40.856080Z",
     "shell.execute_reply.started": "2022-09-28T17:12:40.856080Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "from spacy import Language\n",
    "from typing import List\n",
    "\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def call_wiki_api(item):\n",
    "    try:\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "        data = requests.get(url).json()\n",
    "        # Return the first id (Could upgrade this in the future)\n",
    "        return data['search'][0]['id']\n",
    "    except:\n",
    "        return 'id-less'\n",
    "\n",
    "def extract_triplets(text):\n",
    "    \"\"\"\n",
    "    Function to parse the generated text and extract the triplets\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": 0,\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "            Doc.set_extension(\"rel\", default={})\n",
    "\n",
    "    def get_wiki_id(self, item: str):\n",
    "        mapping = self.entity_mapping.get(item)\n",
    "        if mapping:\n",
    "            return mapping\n",
    "        else:\n",
    "            res = call_wiki_api(item)\n",
    "            self.entity_mapping[item] = res\n",
    "            return res\n",
    "\n",
    "    \n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "        output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "        extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "        extracted_triplets = extract_triplets(extracted_text[0])\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "                continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746146cc-0242-41fe-912e-4431668ff2a0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-28T17:12:40.857581Z",
     "iopub.status.idle": "2022-09-28T17:12:40.857581Z",
     "shell.execute_reply": "2022-09-28T17:12:40.857581Z",
     "shell.execute_reply.started": "2022-09-28T17:12:40.857581Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = -1 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})\n",
    "\n",
    "# Define rel extraction model\n",
    "\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", config={\n",
    "    'device':DEVICE, # Number of the GPU, -1 if want to use CPU\n",
    "    'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95a45f-4f6f-4c52-8d84-9437d8543de3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-28T17:12:40.858579Z",
     "iopub.status.idle": "2022-09-28T17:12:40.859582Z",
     "shell.execute_reply": "2022-09-28T17:12:40.859081Z",
     "shell.execute_reply.started": "2022-09-28T17:12:40.859081Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"Christian Drosten works in Germany. He likes to work for Google.\"\n",
    "\n",
    "coref_text = coref(input_text)._.resolved_text\n",
    "\n",
    "doc = rel_ext(coref_text)\n",
    "\n",
    "for value, rel_dict in doc._.rel.items():\n",
    "    print(f\"{value}: {rel_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940b823-c364-45f6-887d-ab6fa44719e0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-28T17:12:40.861080Z",
     "iopub.status.idle": "2022-09-28T17:12:40.861581Z",
     "shell.execute_reply": "2022-09-28T17:12:40.861581Z",
     "shell.execute_reply.started": "2022-09-28T17:12:40.861581Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Define Neo4j connection\n",
    "host = 'bolt://3.236.134.179:7687'\n",
    "user = 'neo4j'\n",
    "password = 'writer-calibers-steels'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))\n",
    "\n",
    "import_query = \"\"\"\n",
    "UNWIND $data AS row\n",
    "MERGE (h:Entity {id: CASE WHEN NOT row.head_span.id = 'id-less' THEN row.head_span.id ELSE row.head_span.text END})\n",
    "ON CREATE SET h.text = row.head_span.text\n",
    "MERGE (t:Entity {id: CASE WHEN NOT row.tail_span.id = 'id-less' THEN row.tail_span.id ELSE row.tail_span.text END})\n",
    "ON CREATE SET t.text = row.tail_span.text\n",
    "WITH row, h, t\n",
    "CALL apoc.merge.relationship(h, toUpper(replace(row.relation,' ', '_')),\n",
    "  {},\n",
    "  {},\n",
    "  t,\n",
    "  {}\n",
    ")\n",
    "YIELD rel\n",
    "RETURN distinct 'done' AS result;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_query(query, params={}):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], columns=result.keys())\n",
    "\n",
    "def store_wikipedia_summary(page):\n",
    "    try:\n",
    "        input_text = wikipedia.page(page).summary\n",
    "        coref_text = coref(input_text)._.resolved_text\n",
    "        doc = rel_ext(coref_text)\n",
    "        params = [rel_dict for value, rel_dict in doc._.rel.items()]\n",
    "        run_query(import_query, {'data': params})\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't parse text for {page} due to {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e7a82-0202-4dd1-8268-90285988a1d0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-28T17:12:40.862580Z",
     "iopub.status.idle": "2022-09-28T17:12:40.863079Z",
     "shell.execute_reply": "2022-09-28T17:12:40.863079Z",
     "shell.execute_reply.started": "2022-09-28T17:12:40.863079Z"
    }
   },
   "outputs": [],
   "source": [
    "ladies = [\"Jennifer Doudna\", \"Rachel Carson\", \"Sara Seager OC\", \"Gertrude Elion\", \"Rita Levi-Montalcini\"]\n",
    "\n",
    "for l in ladies:\n",
    "    print(f\"Parsing {l}\")\n",
    "    store_wikipedia_summary(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce37541-51a3-4820-b832-9c41e83821ed",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-28T17:12:40.864079Z",
     "iopub.status.idle": "2022-09-28T17:12:40.864581Z",
     "shell.execute_reply": "2022-09-28T17:12:40.864581Z",
     "shell.execute_reply.started": "2022-09-28T17:12:40.864581Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_query(\"\"\"\n",
    "CALL apoc.periodic.iterate(\"\n",
    "  MATCH (e:Entity)\n",
    "  WHERE e.id STARTS WITH 'Q'\n",
    "  RETURN e\n",
    "\",\"\n",
    "  // Prepare a SparQL query\n",
    "  WITH 'SELECT * WHERE{ ?item rdfs:label ?name . filter (?item = wd:' + e.id + ') filter (lang(?name) = \\\\\\\"en\\\\\\\") ' +\n",
    "     'OPTIONAL {?item wdt:P31 [rdfs:label ?label] .filter(lang(?label)=\\\\\\\"en\\\\\\\")}}' AS sparql, e\n",
    "  // make a request to Wikidata\n",
    "  CALL apoc.load.jsonParams(\n",
    "    'https://query.wikidata.org/sparql?query=' + \n",
    "      sparql,\n",
    "      { Accept: 'application/sparql-results+json'}, null)\n",
    "  YIELD value\n",
    "  UNWIND value['results']['bindings'] as row\n",
    "  SET e.wikipedia_name = row.name.value\n",
    "  WITH e, row.label.value AS label\n",
    "  MERGE (c:Class {id:label})\n",
    "  MERGE (e)-[:INSTANCE_OF]->(c)\n",
    "  RETURN distinct 'done'\", {batchSize:1, retry:1})\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
